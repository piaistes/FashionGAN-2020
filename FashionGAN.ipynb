{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FashionGAN",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPYqbQy5EhZDPebyQP23km2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7a9c5671bfdd46b1a28bf98e9fd0cf00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_173aa373f9194b969f035dcf39801591",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b5251579f0ff46299a38ece963ea3323",
              "IPY_MODEL_99c4e05711874b65b3b5c6cfef5c63b2"
            ]
          }
        },
        "173aa373f9194b969f035dcf39801591": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b5251579f0ff46299a38ece963ea3323": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e2fdb61d9d5642c7a4f5bc1ec461e335",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Saving images: 10",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_81d0308d2808450e8260d790389a977f"
          }
        },
        "99c4e05711874b65b3b5c6cfef5c63b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a94a32d9977c4c94b554b059cd9f7dd8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 10,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 10,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c4db32038e2040b9a80864fececafeec"
          }
        },
        "e2fdb61d9d5642c7a4f5bc1ec461e335": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "81d0308d2808450e8260d790389a977f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a94a32d9977c4c94b554b059cd9f7dd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c4db32038e2040b9a80864fececafeec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/piaistes/FashionGAN-2020/blob/main/FashionGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbxSnIJ5wCno"
      },
      "source": [
        "\r\n",
        "# **FASHION GAN 2020**\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFRe4p6kuNzK"
      },
      "source": [
        "This script was part of a thesis by Pia Jabs at the Institute for Marketing and Customer Insight, University of Hamburg (12/2020).\r\n",
        "It is based on StyleGAN2 and partially consists of code snippets published by other artists, which are declared as such in the following. \r\n",
        "To be able to run this code, you need to have your folder with 128x128 images stored on your Google-Drive. I also recommend to have at least 50GB of memory available on your Google-Drive. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f3jtm1KwmWZ"
      },
      "source": [
        "##Set-Up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVLNfGMYiLKo"
      },
      "source": [
        "Change tensorflow version to 1, as StyleGAN2 is configured for only this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dP_CjJdhdCpO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28392fa9-18a0-47f3-b3b7-5c3220be5a00"
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lx35UMa1q8DV"
      },
      "source": [
        "Load package 'time' to keep track of processing times."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "778HxWzDu_JI"
      },
      "source": [
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9H-PdO6JiS7x"
      },
      "source": [
        "Connect Colab to your Google Drive to access more memory and to save files permanentely (files in Colab will be deleted once the session is interrupted/closed)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUyRsdI4BWQl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f6fb122-ad90-44a1-d2fc-b0781212d8a8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XttkluGiXaR"
      },
      "source": [
        "Clone NVIDIA StyleGAN2 Github Repository."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Za9uNtfPBvJw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "96d5435b-10e1-4b6f-e7a1-ee8dbb44c7b0"
      },
      "source": [
        "!git clone https://github.com/NVlabs/stylegan2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'stylegan2'...\n",
            "remote: Enumerating objects: 93, done.\u001b[K\n",
            "remote: Total 93 (delta 0), reused 0 (delta 0), pack-reused 93\u001b[K\n",
            "Unpacking objects: 100% (93/93), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h03w5QvRiDoJ"
      },
      "source": [
        "Check if repository was cloned successfully."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zD_Lkq5dh438",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "4f8a7ce7-c162-4484-ef0b-620ca31587f0"
      },
      "source": [
        "!ls /content/drive/My\\ Drive/stylegan2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "datasets\t LICENSE.txt\t\t __pycache__\t   run_training.py\n",
            "dataset_tool.py  metrics\t\t README.md\t   test_nvcc.cu\n",
            "dnnlib\t\t preprocessedImages128\t run_generator.py  training\n",
            "Dockerfile\t pretrained_networks.py  run_metrics.py\n",
            "docs\t\t projector.py\t\t run_projector.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-uwODAOxZfs"
      },
      "source": [
        "\r\n",
        "\r\n",
        "---\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "punr1mcz68oz"
      },
      "source": [
        "##Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tur7SLZ9Ow_p"
      },
      "source": [
        "Update path to directory of latest .pkl file to ensure transfer learning after training was interrupted. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROXa5VVTUhYU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a66206f7-f1b0-4047-9ca0-dada1bda362d"
      },
      "source": [
        "%cd /content/drive/My\\ Drive/Data\\ Masterarbeit/resultsV1/00022-stylegan2-V1TF-1gpu-config-f"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Data Masterarbeit/resultsV1/00022-stylegan2-V1TF-1gpu-config-f\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Jqg54BCfZG_"
      },
      "source": [
        "The following code is based on https://github.com/NVlabs/stylegan2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qG8MBOvripFA"
      },
      "source": [
        "Load 128x128 Images from your Google-Drive to change format to tf.records (insert path where to store tfrecords and path to original image folder)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZwLmeM_CfIv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "c3315c34-6fbc-41c0-e9b0-a1739092f7c6"
      },
      "source": [
        "start = time.perf_counter()\n",
        "!python /content/drive/My\\ Drive/stylegan2/dataset_tool.py create_from_images /content/drive/My\\ Drive/stylegan2/datasets/V1TF /content/drive/My\\ Drive/Data\\ Masterarbeit/preprocessedV1\n",
        "end = time.perf_counter()\n",
        "print(f\"Transformation of images into tf.records in {end - start:0.4f} seconds.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading images from \"/content/drive/My Drive/Data Masterarbeit/preprocessedV1\"\n",
            "Creating dataset \"/content/drive/My Drive/stylegan2/datasets/V1TF\"\n",
            "Added 14434 images.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuPdw0J0i0LB"
      },
      "source": [
        "Training (insert data-directory of tfrecords, dataset = name of tfrecords dataset, path to result-directory)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMEpgvoewAuY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "29e8bf89-1b07-48b6-d04d-62c419788536"
      },
      "source": [
        "!python /content/drive/My\\ Drive/stylegan2/run_training.py --num-gpus=1 --data-dir=/content/drive/My\\ Drive/stylegan2/datasets --config=config-f \\\n",
        "  --dataset=V1TF --result-dir=/content/drive/My\\ Drive/Data\\ Masterarbeit/resultsV1 --mirror-augment=true --metrics=none"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Local submit - run_dir: /content/drive/My Drive/Data Masterarbeit/resultsV1/00022-stylegan2-V1TF-1gpu-config-f\n",
            "dnnlib: Running training.training_loop.training_loop() on localhost...\n",
            "Streaming data using training.dataset.TFRecordDataset...\n",
            "tcmalloc: large alloc 4294967296 bytes == 0x71f8000 @  0x7f97bed83001 0x7f97bb80f765 0x7f97bb873bb0 0x7f97bb875a4f 0x7f97bb90c048 0x50a7f5 0x50cfd6 0x507f24 0x509202 0x594b01 0x54a17f 0x5517c1 0x59fe1e 0x50d596 0x507f24 0x588fac 0x59fe1e 0x50d596 0x507f24 0x588fac 0x59fe1e 0x50d596 0x509918 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4 0x507f24 0x588fac 0x59fe1e\n",
            "tcmalloc: large alloc 4294967296 bytes == 0x7f960e41e000 @  0x7f97bed811e7 0x7f97bb80f5e1 0x7f97bb873c78 0x7f97bb873f37 0x7f97bb90bf28 0x50a7f5 0x50cfd6 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x509918 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x507f24\n",
            "tcmalloc: large alloc 4294967296 bytes == 0x7f950d41c000 @  0x7f97bed811e7 0x7f97bb80f5e1 0x7f97bb873c78 0x7f97bb873f37 0x7f977dca50c5 0x7f977d628902 0x7f977d628eb2 0x7f977d5e1c3e 0x50a47f 0x50c1f4 0x509918 0x50a64d 0x50c1f4 0x507f24 0x588ddb 0x59fe1e 0x50d596 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4 0x507f24 0x509202 0x594b01\n",
            "Dataset shape = [3, 128, 128]\n",
            "Dynamic range = [0, 255]\n",
            "Label size    = 0\n",
            "Loading networks from \"network-snapshot-000012.pkl\"...\n",
            "Setting up TensorFlow plugin \"fused_bias_act.cu\": Preprocessing... Loading... Done.\n",
            "Setting up TensorFlow plugin \"upfirdn_2d.cu\": Preprocessing... Loading... Done.\n",
            "\n",
            "G                             Params    OutputShape         WeightShape     \n",
            "---                           ---       ---                 ---             \n",
            "latents_in                    -         (?, 512)            -               \n",
            "labels_in                     -         (?, 0)              -               \n",
            "lod                           -         ()                  -               \n",
            "dlatent_avg                   -         (512,)              -               \n",
            "G_mapping/latents_in          -         (?, 512)            -               \n",
            "G_mapping/labels_in           -         (?, 0)              -               \n",
            "G_mapping/Normalize           -         (?, 512)            -               \n",
            "G_mapping/Dense0              262656    (?, 512)            (512, 512)      \n",
            "G_mapping/Dense1              262656    (?, 512)            (512, 512)      \n",
            "G_mapping/Dense2              262656    (?, 512)            (512, 512)      \n",
            "G_mapping/Dense3              262656    (?, 512)            (512, 512)      \n",
            "G_mapping/Dense4              262656    (?, 512)            (512, 512)      \n",
            "G_mapping/Dense5              262656    (?, 512)            (512, 512)      \n",
            "G_mapping/Dense6              262656    (?, 512)            (512, 512)      \n",
            "G_mapping/Dense7              262656    (?, 512)            (512, 512)      \n",
            "G_mapping/Broadcast           -         (?, 12, 512)        -               \n",
            "G_mapping/dlatents_out        -         (?, 12, 512)        -               \n",
            "Truncation/Lerp               -         (?, 12, 512)        -               \n",
            "G_synthesis/dlatents_in       -         (?, 12, 512)        -               \n",
            "G_synthesis/4x4/Const         8192      (?, 512, 4, 4)      (1, 512, 4, 4)  \n",
            "G_synthesis/4x4/Conv          2622465   (?, 512, 4, 4)      (3, 3, 512, 512)\n",
            "G_synthesis/4x4/ToRGB         264195    (?, 3, 4, 4)        (1, 1, 512, 3)  \n",
            "G_synthesis/8x8/Conv0_up      2622465   (?, 512, 8, 8)      (3, 3, 512, 512)\n",
            "G_synthesis/8x8/Conv1         2622465   (?, 512, 8, 8)      (3, 3, 512, 512)\n",
            "G_synthesis/8x8/Upsample      -         (?, 3, 8, 8)        -               \n",
            "G_synthesis/8x8/ToRGB         264195    (?, 3, 8, 8)        (1, 1, 512, 3)  \n",
            "G_synthesis/16x16/Conv0_up    2622465   (?, 512, 16, 16)    (3, 3, 512, 512)\n",
            "G_synthesis/16x16/Conv1       2622465   (?, 512, 16, 16)    (3, 3, 512, 512)\n",
            "G_synthesis/16x16/Upsample    -         (?, 3, 16, 16)      -               \n",
            "G_synthesis/16x16/ToRGB       264195    (?, 3, 16, 16)      (1, 1, 512, 3)  \n",
            "G_synthesis/32x32/Conv0_up    2622465   (?, 512, 32, 32)    (3, 3, 512, 512)\n",
            "G_synthesis/32x32/Conv1       2622465   (?, 512, 32, 32)    (3, 3, 512, 512)\n",
            "G_synthesis/32x32/Upsample    -         (?, 3, 32, 32)      -               \n",
            "G_synthesis/32x32/ToRGB       264195    (?, 3, 32, 32)      (1, 1, 512, 3)  \n",
            "G_synthesis/64x64/Conv0_up    2622465   (?, 512, 64, 64)    (3, 3, 512, 512)\n",
            "G_synthesis/64x64/Conv1       2622465   (?, 512, 64, 64)    (3, 3, 512, 512)\n",
            "G_synthesis/64x64/Upsample    -         (?, 3, 64, 64)      -               \n",
            "G_synthesis/64x64/ToRGB       264195    (?, 3, 64, 64)      (1, 1, 512, 3)  \n",
            "G_synthesis/128x128/Conv0_up  1442561   (?, 256, 128, 128)  (3, 3, 512, 256)\n",
            "G_synthesis/128x128/Conv1     721409    (?, 256, 128, 128)  (3, 3, 256, 256)\n",
            "G_synthesis/128x128/Upsample  -         (?, 3, 128, 128)    -               \n",
            "G_synthesis/128x128/ToRGB     132099    (?, 3, 128, 128)    (1, 1, 256, 3)  \n",
            "G_synthesis/images_out        -         (?, 3, 128, 128)    -               \n",
            "G_synthesis/noise0            -         (1, 1, 4, 4)        -               \n",
            "G_synthesis/noise1            -         (1, 1, 8, 8)        -               \n",
            "G_synthesis/noise2            -         (1, 1, 8, 8)        -               \n",
            "G_synthesis/noise3            -         (1, 1, 16, 16)      -               \n",
            "G_synthesis/noise4            -         (1, 1, 16, 16)      -               \n",
            "G_synthesis/noise5            -         (1, 1, 32, 32)      -               \n",
            "G_synthesis/noise6            -         (1, 1, 32, 32)      -               \n",
            "G_synthesis/noise7            -         (1, 1, 64, 64)      -               \n",
            "G_synthesis/noise8            -         (1, 1, 64, 64)      -               \n",
            "G_synthesis/noise9            -         (1, 1, 128, 128)    -               \n",
            "G_synthesis/noise10           -         (1, 1, 128, 128)    -               \n",
            "images_out                    -         (?, 3, 128, 128)    -               \n",
            "---                           ---       ---                 ---             \n",
            "Total                         29328669                                      \n",
            "\n",
            "\n",
            "D                    Params    OutputShape         WeightShape     \n",
            "---                  ---       ---                 ---             \n",
            "images_in            -         (?, 3, 128, 128)    -               \n",
            "labels_in            -         (?, 0)              -               \n",
            "128x128/FromRGB      1024      (?, 256, 128, 128)  (1, 1, 3, 256)  \n",
            "128x128/Conv0        590080    (?, 256, 128, 128)  (3, 3, 256, 256)\n",
            "128x128/Conv1_down   1180160   (?, 512, 64, 64)    (3, 3, 256, 512)\n",
            "128x128/Skip         131072    (?, 512, 64, 64)    (1, 1, 256, 512)\n",
            "64x64/Conv0          2359808   (?, 512, 64, 64)    (3, 3, 512, 512)\n",
            "64x64/Conv1_down     2359808   (?, 512, 32, 32)    (3, 3, 512, 512)\n",
            "64x64/Skip           262144    (?, 512, 32, 32)    (1, 1, 512, 512)\n",
            "32x32/Conv0          2359808   (?, 512, 32, 32)    (3, 3, 512, 512)\n",
            "32x32/Conv1_down     2359808   (?, 512, 16, 16)    (3, 3, 512, 512)\n",
            "32x32/Skip           262144    (?, 512, 16, 16)    (1, 1, 512, 512)\n",
            "16x16/Conv0          2359808   (?, 512, 16, 16)    (3, 3, 512, 512)\n",
            "16x16/Conv1_down     2359808   (?, 512, 8, 8)      (3, 3, 512, 512)\n",
            "16x16/Skip           262144    (?, 512, 8, 8)      (1, 1, 512, 512)\n",
            "8x8/Conv0            2359808   (?, 512, 8, 8)      (3, 3, 512, 512)\n",
            "8x8/Conv1_down       2359808   (?, 512, 4, 4)      (3, 3, 512, 512)\n",
            "8x8/Skip             262144    (?, 512, 4, 4)      (1, 1, 512, 512)\n",
            "4x4/MinibatchStddev  -         (?, 513, 4, 4)      -               \n",
            "4x4/Conv             2364416   (?, 512, 4, 4)      (3, 3, 513, 512)\n",
            "4x4/Dense0           4194816   (?, 512)            (8192, 512)     \n",
            "Output               513       (?, 1)              (512, 1)        \n",
            "scores_out           -         (?, 1)              -               \n",
            "---                  ---       ---                 ---             \n",
            "Total                28389121                                      \n",
            "\n",
            "Building TensorFlow graph...\n",
            "Initializing logs...\n",
            "Training for 25000 kimg...\n",
            "\n",
            "tick 0     kimg 0.1      lod 0.00  minibatch 16   time 1m 08s       sec/tick 67.9    sec/kimg 1060.52 maintenance 0.0    gpumem 4.6\n",
            "tick 1     kimg 12.1     lod 0.00  minibatch 16   time 2h 13m 42s   sec/tick 7871.5  sec/kimg 654.21  maintenance 82.7   gpumem 4.6\n",
            "tick 2     kimg 24.1     lod 0.00  minibatch 16   time 4h 25m 39s   sec/tick 7847.9  sec/kimg 652.25  maintenance 69.4   gpumem 4.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKwyVPxVqqP3"
      },
      "source": [
        "Calculate Metrics (insert path for data-directory for tfrecords, path to latest network pkl file, dataset = name of tfrecords dataset, path to result-directory)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-j9X_EVzqowF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "376b0ec4-5bf3-4873-83d5-dbe263fb6ae5"
      },
      "source": [
        "start = time.perf_counter()\n",
        "!python /content/drive/My\\ Drive/stylegan2/run_metrics.py --data-dir=/content/drive/My\\ Drive/stylegan2/datasets --network=/content/drive/My\\ Drive/Data\\ Masterarbeit/resultsV1/00008-stylegan2-V1TF-1gpu-config-f/network-snapshot-000000.pkl --metrics=fid50k --dataset=V1TF --result-dir=/content/drive/My\\ Drive/Data\\ Masterarbeit/resultsV1\n",
        "end = time.perf_counter()\n",
        "print(f\"Calculating Metrics in {end - start:0.4f} seconds.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Local submit - run_dir: /content/drive/My Drive/Data Masterarbeit/resultsV1/00050-run-metrics\n",
            "dnnlib: Running run_metrics.run() on localhost...\n",
            "Evaluating metrics \"fid50k\" for \"/content/drive/My Drive/Data Masterarbeit/resultsV1/00008-stylegan2-V1TF-1gpu-config-f/network-snapshot-000000.pkl\"...\n",
            "Setting up TensorFlow plugin \"fused_bias_act.cu\": Preprocessing... Loading... Done.\n",
            "Setting up TensorFlow plugin \"upfirdn_2d.cu\": Preprocessing... Loading... Done.\n",
            "Downloading http://d36zk2xti64re0.cloudfront.net/stylegan1/networks/metrics/inception_v3_features.pkl ... done\n",
            "tcmalloc: large alloc 4294967296 bytes == 0x583ea000 @  0x7f207ba6c001 0x7f20784f8765 0x7f207855cbb0 0x7f207855ea4f 0x7f20785f5048 0x50a7f5 0x50cfd6 0x507f24 0x509202 0x594b01 0x54a17f 0x5517c1 0x59fe1e 0x50d596 0x507f24 0x588fac 0x59fe1e 0x50d596 0x509918 0x50a64d 0x50c1f4 0x58e809 0x4b5d7f 0x50c467 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x507f24 0x509202 0x594b01\n",
            "tcmalloc: large alloc 4294967296 bytes == 0x7f1e66000000 @  0x7f207ba6a1e7 0x7f20784f85e1 0x7f207855cc78 0x7f207855cf37 0x7f20785f4f28 0x50a7f5 0x50cfd6 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x509918 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x507f24\n",
            "tcmalloc: large alloc 4294967296 bytes == 0x7f1e66000000 @  0x7f207ba6a1e7 0x7f20784f85e1 0x7f207855cc78 0x7f207855cf37 0x7f203b2100c5 0x7f203ab93902 0x7f203ab93eb2 0x7f203ab4cc3e 0x50a47f 0x50c1f4 0x509918 0x50a64d 0x50c1f4 0x507f24 0x588ddb 0x59fe1e 0x50d596 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4 0x507f24 0x509202 0x594b01\n",
            "network-snapshot-000000        time 33m 16s      fid50k 353.5956\n",
            "dnnlib: Finished run_metrics.run() in 33m 19s.\n",
            "Calculating Metrics in 2016.8136 seconds.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Evcyj8JsP-rC"
      },
      "source": [
        "Generate 100 Images (insert insert path to network pkl file, path to result directory)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqb3xe83PsmL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4da2d4eb-6d15-4662-b977-6d1fcc4d2e28"
      },
      "source": [
        "!python /content/drive/My\\ Drive/stylegan2/run_generator.py generate-images --network=/content/drive/My\\ Drive/Data\\ Masterarbeit/resultsV1/00022-stylegan2-V1TF-1gpu-config-f/network-snapshot-000024.pkl --seeds=0-99 --result-dir=/content/drive/My\\ Drive/Data\\ Masterarbeit/resultsV1/00022-stylegan2-V1TF-1gpu-config-f/GeneratedImages99"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Local submit - run_dir: /content/drive/My Drive/Data Masterarbeit/resultsV1/00022-stylegan2-V1TF-1gpu-config-f/GeneratedImages99/00002-generate-images\n",
            "dnnlib: Running run_generator.generate_images() on localhost...\n",
            "Loading networks from \"/content/drive/My Drive/Data Masterarbeit/resultsV1/00022-stylegan2-V1TF-1gpu-config-f/network-snapshot-000024.pkl\"...\n",
            "Setting up TensorFlow plugin \"fused_bias_act.cu\": Preprocessing... Loading... Done.\n",
            "Setting up TensorFlow plugin \"upfirdn_2d.cu\": Preprocessing... Loading... Done.\n",
            "Generating image for seed 0 (0/100) ...\n",
            "Generating image for seed 1 (1/100) ...\n",
            "Generating image for seed 2 (2/100) ...\n",
            "Generating image for seed 3 (3/100) ...\n",
            "Generating image for seed 4 (4/100) ...\n",
            "Generating image for seed 5 (5/100) ...\n",
            "Generating image for seed 6 (6/100) ...\n",
            "Generating image for seed 7 (7/100) ...\n",
            "Generating image for seed 8 (8/100) ...\n",
            "Generating image for seed 9 (9/100) ...\n",
            "Generating image for seed 10 (10/100) ...\n",
            "Generating image for seed 11 (11/100) ...\n",
            "Generating image for seed 12 (12/100) ...\n",
            "Generating image for seed 13 (13/100) ...\n",
            "Generating image for seed 14 (14/100) ...\n",
            "Generating image for seed 15 (15/100) ...\n",
            "Generating image for seed 16 (16/100) ...\n",
            "Generating image for seed 17 (17/100) ...\n",
            "Generating image for seed 18 (18/100) ...\n",
            "Generating image for seed 19 (19/100) ...\n",
            "Generating image for seed 20 (20/100) ...\n",
            "Generating image for seed 21 (21/100) ...\n",
            "Generating image for seed 22 (22/100) ...\n",
            "Generating image for seed 23 (23/100) ...\n",
            "Generating image for seed 24 (24/100) ...\n",
            "Generating image for seed 25 (25/100) ...\n",
            "Generating image for seed 26 (26/100) ...\n",
            "Generating image for seed 27 (27/100) ...\n",
            "Generating image for seed 28 (28/100) ...\n",
            "Generating image for seed 29 (29/100) ...\n",
            "Generating image for seed 30 (30/100) ...\n",
            "Generating image for seed 31 (31/100) ...\n",
            "Generating image for seed 32 (32/100) ...\n",
            "Generating image for seed 33 (33/100) ...\n",
            "Generating image for seed 34 (34/100) ...\n",
            "Generating image for seed 35 (35/100) ...\n",
            "Generating image for seed 36 (36/100) ...\n",
            "Generating image for seed 37 (37/100) ...\n",
            "Generating image for seed 38 (38/100) ...\n",
            "Generating image for seed 39 (39/100) ...\n",
            "Generating image for seed 40 (40/100) ...\n",
            "Generating image for seed 41 (41/100) ...\n",
            "Generating image for seed 42 (42/100) ...\n",
            "Generating image for seed 43 (43/100) ...\n",
            "Generating image for seed 44 (44/100) ...\n",
            "Generating image for seed 45 (45/100) ...\n",
            "Generating image for seed 46 (46/100) ...\n",
            "Generating image for seed 47 (47/100) ...\n",
            "Generating image for seed 48 (48/100) ...\n",
            "Generating image for seed 49 (49/100) ...\n",
            "Generating image for seed 50 (50/100) ...\n",
            "Generating image for seed 51 (51/100) ...\n",
            "Generating image for seed 52 (52/100) ...\n",
            "Generating image for seed 53 (53/100) ...\n",
            "Generating image for seed 54 (54/100) ...\n",
            "Generating image for seed 55 (55/100) ...\n",
            "Generating image for seed 56 (56/100) ...\n",
            "Generating image for seed 57 (57/100) ...\n",
            "Generating image for seed 58 (58/100) ...\n",
            "Generating image for seed 59 (59/100) ...\n",
            "Generating image for seed 60 (60/100) ...\n",
            "Generating image for seed 61 (61/100) ...\n",
            "Generating image for seed 62 (62/100) ...\n",
            "Generating image for seed 63 (63/100) ...\n",
            "Generating image for seed 64 (64/100) ...\n",
            "Generating image for seed 65 (65/100) ...\n",
            "Generating image for seed 66 (66/100) ...\n",
            "Generating image for seed 67 (67/100) ...\n",
            "Generating image for seed 68 (68/100) ...\n",
            "Generating image for seed 69 (69/100) ...\n",
            "Generating image for seed 70 (70/100) ...\n",
            "Generating image for seed 71 (71/100) ...\n",
            "Generating image for seed 72 (72/100) ...\n",
            "Generating image for seed 73 (73/100) ...\n",
            "Generating image for seed 74 (74/100) ...\n",
            "Generating image for seed 75 (75/100) ...\n",
            "Generating image for seed 76 (76/100) ...\n",
            "Generating image for seed 77 (77/100) ...\n",
            "Generating image for seed 78 (78/100) ...\n",
            "Generating image for seed 79 (79/100) ...\n",
            "Generating image for seed 80 (80/100) ...\n",
            "Generating image for seed 81 (81/100) ...\n",
            "Generating image for seed 82 (82/100) ...\n",
            "Generating image for seed 83 (83/100) ...\n",
            "Generating image for seed 84 (84/100) ...\n",
            "Generating image for seed 85 (85/100) ...\n",
            "Generating image for seed 86 (86/100) ...\n",
            "Generating image for seed 87 (87/100) ...\n",
            "Generating image for seed 88 (88/100) ...\n",
            "Generating image for seed 89 (89/100) ...\n",
            "Generating image for seed 90 (90/100) ...\n",
            "Generating image for seed 91 (91/100) ...\n",
            "Generating image for seed 92 (92/100) ...\n",
            "Generating image for seed 93 (93/100) ...\n",
            "Generating image for seed 94 (94/100) ...\n",
            "Generating image for seed 95 (95/100) ...\n",
            "Generating image for seed 96 (96/100) ...\n",
            "Generating image for seed 97 (97/100) ...\n",
            "Generating image for seed 98 (98/100) ...\n",
            "Generating image for seed 99 (99/100) ...\n",
            "dnnlib: Finished run_generator.generate_images() in 41s.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hiAa9qv2zgm"
      },
      "source": [
        "## `Manipulation`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHug_YAb_o7Q"
      },
      "source": [
        "Change pathes in the following accordingly to training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oItKG2SA8A0l"
      },
      "source": [
        "Change Truncation-Psi for selected Images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szCz3BgE8zSZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "cf838a6f-5e78-4fa6-c2c3-e35d69365abb"
      },
      "source": [
        "!python /content/drive/My\\ Drive/stylegan2/run_generator.py generate-images --network=/content/drive/My\\ Drive/Data\\ Masterarbeit/resultsV1/00022-stylegan2-V1TF-1gpu-config-f/network-snapshot-000024.pkl --seeds=21 --truncation-psi=1 --result-dir=/content/drive/My\\ Drive/Data\\ Masterarbeit/resultsV1/00022-stylegan2-V1TF-1gpu-config-f/GeneratedImages99"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Local submit - run_dir: /content/drive/My Drive/Data Masterarbeit/resultsV1/00022-stylegan2-V1TF-1gpu-config-f/GeneratedImages99/00007-generate-images\n",
            "dnnlib: Running run_generator.generate_images() on localhost...\n",
            "Loading networks from \"/content/drive/My Drive/Data Masterarbeit/resultsV1/00022-stylegan2-V1TF-1gpu-config-f/network-snapshot-000024.pkl\"...\n",
            "Setting up TensorFlow plugin \"fused_bias_act.cu\": Preprocessing... Loading... Done.\n",
            "Setting up TensorFlow plugin \"upfirdn_2d.cu\": Preprocessing... Loading... Done.\n",
            "Generating image for seed 21 (0/1) ...\n",
            "dnnlib: Finished run_generator.generate_images() in 14s.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FY-HRRYFAQV2"
      },
      "source": [
        "Transformation of real images into tf.records. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlonVH5yAPen",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "ea958d25-e546-45dc-c530-2753f1cc38d4"
      },
      "source": [
        "!python /content/drive/My\\ Drive/stylegan2/dataset_tool.py create_from_images /content/drive/My\\ Drive/stylegan2/datasets/V1ProjektionTF /content/drive/My\\ Drive/Data\\ Masterarbeit/V1Projektion"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading images from \"/content/drive/My Drive/Data Masterarbeit/V1Projektion\"\n",
            "Creating dataset \"/content/drive/My Drive/stylegan2/datasets/V1ProjektionTF\"\n",
            "Added 3 images.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLzdCyzf8JRo"
      },
      "source": [
        "Projection of real images in W."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRDHtIK78O0O",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "416d478f-fe78-49cd-ae70-cec3cabeb86c"
      },
      "source": [
        "!python /content/drive/My\\ Drive/stylegan2/run_projector.py project-real-images --network=/content/drive/My\\ Drive/Data\\ Masterarbeit/resultsV1/00022-stylegan2-V1TF-1gpu-config-f/network-snapshot-000024.pkl \\\n",
        "  --dataset=V1ProjektionTF --data-dir=/content/drive/My\\ Drive/stylegan2/datasets --result-dir=/content/drive/My\\ Drive/Data\\ Masterarbeit/resultsV1/Projektion"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Local submit - run_dir: /content/drive/My Drive/Data Masterarbeit/resultsV1/Projektion/00002-project-real-images\n",
            "dnnlib: Running run_projector.project_real_images() on localhost...\n",
            "Loading networks from \"/content/drive/My Drive/Data Masterarbeit/resultsV1/00022-stylegan2-V1TF-1gpu-config-f/network-snapshot-000024.pkl\"...\n",
            "Setting up TensorFlow plugin \"fused_bias_act.cu\": Preprocessing... Loading... Done.\n",
            "Setting up TensorFlow plugin \"upfirdn_2d.cu\": Preprocessing... Loading... Done.\n",
            "Loading images from \"V1ProjektionTF\"...\n",
            "tcmalloc: large alloc 4294967296 bytes == 0x56812000 @  0x7f0f32d93001 0x7f0f308b7765 0x7f0f3091bbb0 0x7f0f3091da4f 0x7f0f309b4048 0x50a7f5 0x50cfd6 0x507f24 0x509202 0x594b01 0x54a17f 0x5517c1 0x59fe1e 0x50d596 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x507f24 0x588fac 0x59fe1e 0x50d596 0x509918 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4 0x507f24 0x588fac 0x59fe1e\n",
            "tcmalloc: large alloc 4294967296 bytes == 0x7f0d05c40000 @  0x7f0f32d911e7 0x7f0f308b75e1 0x7f0f3091bc78 0x7f0f3091bf37 0x7f0f309b3f28 0x50a7f5 0x50cfd6 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x509918 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x507f24\n",
            "tcmalloc: large alloc 4294967296 bytes == 0x7f0d05c40000 @  0x7f0f32d911e7 0x7f0f308b75e1 0x7f0f3091bc78 0x7f0f3091bf37 0x7f0ef25370c5 0x7f0ef1eba902 0x7f0ef1ebaeb2 0x7f0ef1e73c3e 0x50a47f 0x50c1f4 0x509918 0x50a64d 0x50c1f4 0x507f24 0x588ddb 0x59fe1e 0x50d596 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4 0x507f24 0x509202 0x594b01\n",
            "Projecting image 0/3 ...\n",
            "Projecting image 1/3 ...\n",
            "Projecting image 2/3 ...\n",
            "dnnlib: Finished run_projector.project_real_images() in 5m 55s.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtBzVSAmF6Om"
      },
      "source": [
        "Interpolation  of generated Images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AM-Ltx4pF5pM"
      },
      "source": [
        "!python run_generator.py generate-latent-walk --network=/content/drive/My\\ Drive/Data\\ Masterarbeit/resultsV1/00022-stylegan2-V1TF-1gpu-config-f/network-snapshot-000024.pkl --walk-type='line-w' --seeds=27,28 --frames 10 --truncation-psi=0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Kq_IY2IGDiM"
      },
      "source": [
        "Style-Mixing for generated Images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0X2PgbCawhLW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "93f78d39-6b80-45c9-9495-a004cf0a317a"
      },
      "source": [
        "!python /content/drive/My\\ Drive/stylegan2/run_generator.py style-mixing-example --network=/content/drive/My\\ Drive/Data\\ Masterarbeit/resultsV1/00022-stylegan2-V1TF-1gpu-config-f/network-snapshot-000024.pkl --row-seeds=27,9,69,71,81 --col-seeds=28,39,72,75,85 --col-styles=0,1,2,3,8,9,10,11 --result-dir=/content/drive/My\\ Drive/Data\\ Masterarbeit/resultsV1/Style "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Local submit - run_dir: /content/drive/My Drive/Data Masterarbeit/resultsV1/Style/00033-style-mixing-example\n",
            "dnnlib: Running run_generator.style_mixing_example() on localhost...\n",
            "Loading networks from \"/content/drive/My Drive/Data Masterarbeit/resultsV1/00022-stylegan2-V1TF-1gpu-config-f/network-snapshot-000024.pkl\"...\n",
            "Setting up TensorFlow plugin \"fused_bias_act.cu\": Preprocessing... Loading... Done.\n",
            "Setting up TensorFlow plugin \"upfirdn_2d.cu\": Preprocessing... Loading... Done.\n",
            "Generating W vectors...\n",
            "Generating images...\n",
            "Generating style-mixed images...\n",
            "Saving images...\n",
            "Saving image grid...\n",
            "dnnlib: Finished run_generator.style_mixing_example() in 13s.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCGJUUMwh9In"
      },
      "source": [
        "The following code is based on https://colab.research.google.com/drive/1qGU6NCz-2SI1BSLkFTZq0gYN2DNnXIXm#scrollTo=9oR65UOnNfV- - to additionally allow for manipulation of projected (real) images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7qEmAr5gbJ-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e98ee25c-ed4f-4182-c3ee-86aac93c7224"
      },
      "source": [
        "%cd /content/drive/My\\ Drive/stylegan2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/stylegan2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nwxPWryJJ7G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68dbabf1-f0c4-4882-8d6a-521f2cb8ab4f"
      },
      "source": [
        "!pip install opensimplex"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting opensimplex\n",
            "  Downloading https://files.pythonhosted.org/packages/9c/ad/9b758f9ff9dcd23fc574bb3aa1de844adb1179c9be9711e9f798614d4b2f/opensimplex-0.3-py3-none-any.whl\n",
            "Installing collected packages: opensimplex\n",
            "Successfully installed opensimplex-0.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODHDjUoS-wBm"
      },
      "source": [
        "Insert id to named pkl file (retrieve from Google-Drive Link of .pkl File)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtKc02XYfDaW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbf8d00b-0f6d-47aa-8939-54beb1422ab7"
      },
      "source": [
        "!mkdir pkl\n",
        "!gdown --id 1gbxwfHNOaGjGsLNTmmSrNA85X2VWHHOq -O /content/drive/My\\ Drive/stylegan2/pkl/vgg16_zhang_perceptual.pkl"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory â€˜pklâ€™: File exists\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1gbxwfHNOaGjGsLNTmmSrNA85X2VWHHOq\n",
            "To: /content/drive/My Drive/stylegan2/pkl/vgg16_zhang_perceptual.pkl\n",
            "58.9MB [00:00, 95.8MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGH84iT3_AqZ"
      },
      "source": [
        "Insert path to pkl file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "193qMo8jiNXk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdfb24cb-4120-492a-f38c-85680de8f0ba"
      },
      "source": [
        "network_pkl = \"/content/drive/My Drive/Data Masterarbeit/resultsV3/00048-stylegan2-V3TF-1gpu-config-f/network-snapshot-000008.pkl\"\n",
        "\n",
        "import argparse\n",
        "import PIL.Image\n",
        "import dnnlib\n",
        "import dnnlib.tflib as tflib\n",
        "import re\n",
        "import sys\n",
        "from io import BytesIO\n",
        "import numpy as np\n",
        "from math import ceil\n",
        "from PIL import Image, ImageDraw\n",
        "import imageio\n",
        "\n",
        "import pretrained_networks\n",
        "\n",
        "# Convert uploaded images to TFRecords\n",
        "import dataset_tool\n",
        "\n",
        "# Run the projector\n",
        "import training.dataset\n",
        "import training.misc\n",
        "import projector\n",
        "import os \n",
        "\n",
        "# Taken from https://github.com/alexanderkuk/log-progress\n",
        "def log_progress(sequence, every=1, size=None, name='Items'):\n",
        "    from ipywidgets import IntProgress, HTML, VBox\n",
        "    from IPython.display import display\n",
        "\n",
        "    is_iterator = False\n",
        "    if size is None:\n",
        "        try:\n",
        "            size = len(sequence)\n",
        "        except TypeError:\n",
        "            is_iterator = True\n",
        "    if size is not None:\n",
        "        if every is None:\n",
        "            if size <= 200:\n",
        "                every = 1\n",
        "            else:\n",
        "                every = int(size / 200)     # every 0.5%\n",
        "    else:\n",
        "        assert every is not None, 'sequence is iterator, set every'\n",
        "\n",
        "    if is_iterator:\n",
        "        progress = IntProgress(min=0, max=1, value=1)\n",
        "        progress.bar_style = 'info'\n",
        "    else:\n",
        "        progress = IntProgress(min=0, max=size, value=0)\n",
        "    label = HTML()\n",
        "    box = VBox(children=[label, progress])\n",
        "    display(box)\n",
        "\n",
        "    index = 0\n",
        "    try:\n",
        "        for index, record in enumerate(sequence, 1):\n",
        "            if index == 1 or index % every == 0:\n",
        "                if is_iterator:\n",
        "                    label.value = '{name}: {index} / ?'.format(\n",
        "                        name=name,\n",
        "                        index=index\n",
        "                    )\n",
        "                else:\n",
        "                    progress.value = index\n",
        "                    label.value = u'{name}: {index} / {size}'.format(\n",
        "                        name=name,\n",
        "                        index=index,\n",
        "                        size=size\n",
        "                    )\n",
        "            yield record\n",
        "    except:\n",
        "        progress.bar_style = 'danger'\n",
        "        raise\n",
        "    else:\n",
        "        progress.bar_style = 'success'\n",
        "        progress.value = index\n",
        "        label.value = \"{name}: {index}\".format(\n",
        "            name=name,\n",
        "            index=str(index or '?')\n",
        "        )\n",
        "\n",
        "def interpolate(zs, steps,type='linear'):\n",
        "  out = []\n",
        "  for i in range(len(zs)-1):\n",
        "    c = zs[i+1]-zs[i]\n",
        "\n",
        "    for index in range(steps):\n",
        "      fraction = index/float(steps) # t/d\n",
        "      \n",
        "      # translated from: https://github.com/danro/jquery-easing/blob/master/jquery.easing.js\n",
        "      # see https://easings.net/ for examples\n",
        "      if type == 'linear':\n",
        "        out.append( c * fraction + zs[i] ) # c*(t/d)+b\n",
        "      elif type == 'easeInSine':\n",
        "        out.append( -c * np.cos(fraction * (np.pi/2)) + c + zs[i] ) # -c * Math.cos(t/d * (Math.PI/2)) + c + b\n",
        "      elif type == 'easeOutSine':\n",
        "        out.append( c * np.sin(fraction * (np.pi/2)) + zs[i]) # c * Math.sin(t/d * (Math.PI/2)) + b\n",
        "      elif type == 'easeInOutSine':\n",
        "        out.append(-c/2 * (np.cos(np.pi*fraction) - 1.0) + zs[i]) # -c/2 * (Math.cos(Math.PI*t/d) - 1) + b;\n",
        "      elif type == 'easeInQuad':\n",
        "        out.append(c * fraction * fraction + zs[i]) # c*(t/=d)*t + b;\n",
        "      elif type == 'easeOutQuad':\n",
        "       out.append(-c * fraction * (fraction-2) + zs[i]) # -c *(t/=d)*(t-2) + b;\n",
        "      # elif type == 'easeInOutQuad':\n",
        "      #   if(fraction/2 < 1):\n",
        "      #     out.append( ((c/2)*fraction*fraction) + zs[i]) #if ((t/=d/2) < 1) return c/2*t*t + b;\n",
        "      #   else:\n",
        "\t\t  #     out.append( (-c/2) * ((index-=1)*(index-2) - 1) + zs[i]; #return -c/2 * ((--t)*(t-2) - 1) + b;\n",
        "      else: \n",
        "        out.append( c * fraction + zs[i] ) # c*(t/d)+b\n",
        "  return out\n",
        "\n",
        "def saveImgs(imgs, location):\n",
        "  for idx, img in log_progress(enumerate(imgs), size = len(imgs), name=\"Saving images\"):\n",
        "    file = location + ('%05d.png' % (idx))\n",
        "    img.save(file)\n",
        "\n",
        "def generate_images_in_w_space(dlatents, truncation_psi):\n",
        "    Gs_kwargs = dnnlib.EasyDict()\n",
        "    Gs_kwargs.output_transform = dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=True)\n",
        "    Gs_kwargs.randomize_noise = False\n",
        "    Gs_kwargs.truncation_psi = truncation_psi\n",
        "    dlatent_avg = Gs.get_var('dlatent_avg') # [component]\n",
        "\n",
        "    imgs = []\n",
        "    for row, dlatent in enumerate(dlatents):\n",
        "        #row_dlatents = (dlatent[np.newaxis] - dlatent_avg) * np.reshape(truncation_psi, [-1, 1, 1]) + dlatent_avg\n",
        "        dl = (dlatent-dlatent_avg)*truncation_psi   + dlatent_avg\n",
        "        row_images = Gs.components.synthesis.run(dlatent,  **Gs_kwargs)\n",
        "        imgs.append(PIL.Image.fromarray(row_images[0], 'RGB'))\n",
        "    return imgs  \n",
        "\n",
        "print('Loading networks from \"%s\"...' % network_pkl)\n",
        "_G, _D, Gs = pretrained_networks.load_networks(network_pkl)\n",
        "noise_vars = [var for name, var in Gs.components.synthesis.vars.items() if name.startswith('noise')]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading networks from \"/content/drive/My Drive/Data Masterarbeit/resultsV3/00048-stylegan2-V3TF-1gpu-config-f/network-snapshot-000008.pkl\"...\n",
            "Setting up TensorFlow plugin \"fused_bias_act.cu\": Preprocessing... Loading... Done.\n",
            "Setting up TensorFlow plugin \"upfirdn_2d.cu\": Preprocessing... Loading... Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8amBc1SLoz4n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7b66896-cc51-40f4-dc93-7d5c53a8d02d"
      },
      "source": [
        "!mkdir projection\n",
        "!mkdir projection/imgs\n",
        "!mkdir projection/out"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory â€˜projectionâ€™: File exists\n",
            "mkdir: cannot create directory â€˜projection/imgsâ€™: File exists\n",
            "mkdir: cannot create directory â€˜projection/outâ€™: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-RUuZGkpfCd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a12c2d8-ff7d-457e-8df1-4d5e71877cb4"
      },
      "source": [
        "# Convert uploaded images to TFRecords\n",
        "dataset_tool.create_from_images(\"./projection/records/\", \"./projection/imgs/\", True)\n",
        "\n",
        "def project_image_run(proj, targets, png_prefix, num_snapshots):\n",
        "    snapshot_steps = set(proj.num_steps - np.linspace(0, proj.num_steps, num_snapshots, endpoint=False, dtype=int))\n",
        "    training.misc.save_image_grid(targets, png_prefix + 'target.png', drange=[-1,1])\n",
        "    proj.start(targets)\n",
        "    while proj.get_cur_step() < proj.num_steps:\n",
        "        print('\\r%d / %d ... ' % (proj.get_cur_step(), proj.num_steps), end='', flush=True)\n",
        "        proj.step()\n",
        "        if proj.get_cur_step() in snapshot_steps:\n",
        "            training.misc.save_image_grid(proj.get_images(), png_prefix + 'step%04d.png' % proj.get_cur_step(), drange=[-1,1])\n",
        "    print('\\r%-30s\\r' % '', end='', flush=True)\n",
        "    type(proj.get_noises())\n",
        "    #if you want to do interpolations, name the file below something memorable\n",
        "    file_name = '3'\n",
        "    np.save(('./projection/'+file_name+'.npy'), proj.get_dlatents())\n",
        "\n",
        "def project_real_images(dataset_name, data_dir, num_images, num_snapshots):\n",
        "    proj = projector.Projector()\n",
        "    proj.set_network(Gs)\n",
        "    #num_steps = how many iterations; larger = ~more accurate but longer run times \n",
        "    proj.num_steps=1500\n",
        "\n",
        "    print('Loading images from \"%s\"...' % dataset_name)\n",
        "    dataset_obj = training.dataset.load_dataset(data_dir=data_dir, tfrecord_dir=dataset_name, max_label_size=0, verbose=True, repeat=False, shuffle_mb=0)\n",
        "    print(dataset_obj.shape, Gs.output_shape[1:])\n",
        "    assert dataset_obj.shape == Gs.output_shape[1:]\n",
        "\n",
        "    for image_idx in range(num_images):\n",
        "        print('Projecting image %d/%d ...' % (image_idx, num_images))\n",
        "        images, _labels = dataset_obj.get_minibatch_np(1)\n",
        "        images = training.misc.adjust_dynamic_range(images, [0, 255], [-1, 1])\n",
        "        project_image_run(proj, targets=images, png_prefix=dnnlib.make_run_dir_path('projection/out/image%04d-' % image_idx), num_snapshots=num_snapshots)\n",
        "\n",
        "project_real_images(\"records\",\"./projection\",1,10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading images from \"./projection/imgs/\"\n",
            "Creating dataset \"./projection/records/\"\n",
            "Added 1 images.\n",
            "Loading images from \"records\"...\n",
            "Streaming data using training.dataset.TFRecordDataset...\n",
            "Dataset shape = [3, 256, 256]\n",
            "Dynamic range = [0, 255]\n",
            "Label size    = 0\n",
            "[3, 256, 256] [3, 256, 256]\n",
            "Projecting image 0/1 ...\n",
            "0 / 1500 ... "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iI1lHl3IsgA2"
      },
      "source": [
        "Interpolation of projected images.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJCg9N_jsimG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "7a9c5671bfdd46b1a28bf98e9fd0cf00",
            "173aa373f9194b969f035dcf39801591",
            "b5251579f0ff46299a38ece963ea3323",
            "99c4e05711874b65b3b5c6cfef5c63b2",
            "e2fdb61d9d5642c7a4f5bc1ec461e335",
            "81d0308d2808450e8260d790389a977f",
            "a94a32d9977c4c94b554b059cd9f7dd8",
            "c4db32038e2040b9a80864fececafeec"
          ]
        },
        "outputId": "ffa84b75-9e8c-4905-d290-e466e2e7fdff"
      },
      "source": [
        "latent1 = np.load('/content/drive/My Drive/stylegan2/projection/3.npy')\n",
        "latent2 = np.load('/content/drive/My Drive/stylegan2/projection/4.npy')\n",
        "\n",
        "imgs = generate_images_in_w_space(interpolate([latent1,latent2],10,'linear'),0.5)\n",
        "\n",
        "!rm -rf interpolations\n",
        "%mkdir interpolations\n",
        "saveImgs(imgs,'./interpolations/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7a9c5671bfdd46b1a28bf98e9fd0cf00",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "VBox(children=(HTML(value=''), IntProgress(value=0, max=10)))"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}